{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Lexical Similarity Analysis\n",
    "\n",
    "This notebook demonstrates **lexical similarity** - a weak signal based on token-level text comparison using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.similarity.lexical import LexicalSimilarity\n",
    "from src.io import load_submissions\n",
    "from src.normalization import get_normalizer\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Lexical Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "lexical_analyzer = LexicalSimilarity()\n",
    "\n",
    "#Test codes\n",
    "code1 = '''\n",
    "def factorial(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return n * factorial(n-1)\n",
    "'''\n",
    "\n",
    "code2 = '''\n",
    "def fact(x):\n",
    "    if x <= 1:\n",
    "        return 1\n",
    "    return x * fact(x-1)\n",
    "'''\n",
    "\n",
    "# Compute similarity\n",
    "similarity = lexical_analyzer.compute_similarity(code1, code2)\n",
    "\n",
    "print(\"ðŸ“Š Lexical Similarity Test\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Similarity Score: {similarity:.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Token statistics\n",
    "stats1 = lexical_analyzer.get_token_statistics(code1)\n",
    "stats2 = lexical_analyzer.get_token_statistics(code2)\n",
    "\n",
    "print(\"\\nCode 1 Statistics:\")\n",
    "for key, value in stats1.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nCode 2 Statistics:\")\n",
    "for key, value in stats2.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pairwise Similarity Matrix\n",
    "\n",
    "Compute lexical similarity for all submission pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load submissions\n",
    "submissions = load_submissions('../data/raw/sample_submissions.csv')\n",
    "\n",
    "# Normalize all codes first\n",
    "normalizer = get_normalizer('python')\n",
    "normalized_codes = []\n",
    "for sub in submissions:\n",
    "    normalizer.reset_counters()\n",
    "    normalized_codes.append(normalizer.normalize(sub['code']))\n",
    "\n",
    "# Compute pairwise similarities\n",
    "similarity_matrix = lexical_analyzer.compute_pairwise_similarities(normalized_codes)\n",
    "\n",
    "print(f\"Computed {len(submissions)}x{len(submissions)} similarity matrix\")\n",
    "print(f\"Shape: {similarity_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarity Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap\n",
    "submission_ids = [sub['submission_id'] for sub in submissions]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(similarity_matrix, \n",
    "                 annot=True, \n",
    "                 fmt='.1f',\n",
    "                 cmap='RdYlGn',\n",
    "                 vmin=0, \n",
    "                 vmax=100,\n",
    "                 xticklabels=submission_ids,\n",
    "                 yticklabels=submission_ids,\n",
    "                 cbar_kws={'label': 'Similarity (%)'},\n",
    "                 linewidths=0.5,\n",
    "                 linecolor='gray')\n",
    "\n",
    "plt.title('Lexical Similarity Heatmap (Normalized Code)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Submission ID', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Submission ID', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Rotate labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highest similarity pairs\n",
    "print(\"\\nðŸ” Top 3 Similar Pairs (excluding self):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get upper triangle (excluding diagonal)\n",
    "pairs = []\n",
    "n = len(submissions)\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        pairs.append((submission_ids[i], submission_ids[j], similarity_matrix[i][j]))\n",
    "\n",
    "# Sort by similarity\n",
    "pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for i, (id1, id2, sim) in enumerate(pairs[:3], 1):\n",
    "    print(f\"{i}. {id1} â†” {id2}: {sim:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why Lexical Similarity is a Weak Signal\n",
    "\n",
    "Let's see why we can't rely on lexical similarity alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same algorithm, very different tokens\n",
    "fibonacci_recursive = '''\n",
    "def fib(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)\n",
    "'''\n",
    "\n",
    "fibonacci_iterative = '''\n",
    "def fibonacci(num):\n",
    "    a, b = 0, 1\n",
    "    for i in range(num):\n",
    "        a, b = b, a + b\n",
    "    return a\n",
    "'''\n",
    "\n",
    "# Different algorithm, similar tokens\n",
    "factorial = '''\n",
    "def fib(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return n * fib(n-1)\n",
    "'''\n",
    "\n",
    "# Test 1: Same algorithm, different style\n",
    "sim1 = lexical_analyzer.compute_similarity(fibonacci_recursive, fibonacci_iterative)\n",
    "\n",
    "# Test 2: Different algorithm, similar tokens\n",
    "sim2 = lexical_analyzer.compute_similarity(fibonacci_recursive, factorial)\n",
    "\n",
    "print(\"âš ï¸  Lexical Similarity Weaknesses\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTest 1: Fibonacci (recursive) vs Fibonacci (iterative)\")\n",
    "print(f\"  â†’ Same algorithm, different implementation\")\n",
    "print(f\"  â†’ Lexical similarity: {sim1:.1f}%\")\n",
    "print(f\"  â†’ âŒ LOW score despite being the SAME algorithm!\")\n",
    "\n",
    "print(f\"\\nTest 2: Fibonacci (recursive) vs Factorial (recursive)\")\n",
    "print(f\"  â†’ Different algorithms, similar structure\")\n",
    "print(f\"  â†’ Lexical similarity: {sim2:.1f}%\")\n",
    "print(f\"  â†’ âš ï¸  Could be misleadingly high!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nðŸ’¡ KEY TAKEAWAY:\")\n",
    "print(\"   Lexical similarity can MISS real plagiarism (refactored code)\")\n",
    "print(\"   and give FALSE POSITIVES (similar syntax, different logic).\")\n",
    "print(\"\\n   This is why we need STRUCTURAL and SEMANTIC signals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Before vs After Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with disguised code\n",
    "original = '''\n",
    "def factorial(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    return n * factorial(n-1)\n",
    "'''\n",
    "\n",
    "disguised = '''\n",
    "def compute_factorial(number):\n",
    "    # Base case\n",
    "    if number <= 1:\n",
    "        return 1\n",
    "    # Recursive multiplication\n",
    "    result = number * compute_factorial(number - 1)\n",
    "    return result\n",
    "'''\n",
    "\n",
    "# Before normalization\n",
    "sim_before = lexical_analyzer.compute_similarity(original, disguised)\n",
    "\n",
    "# After normalization\n",
    "normalizer.reset_counters()\n",
    "norm_original = normalizer.normalize(original)\n",
    "normalizer.reset_counters()\n",
    "norm_disguised = normalizer.normalize(disguised)\n",
    "\n",
    "sim_after = lexical_analyzer.compute_similarity(norm_original, norm_disguised)\n",
    "\n",
    "# Visualize improvement\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = ['Before\\nNormalization', 'After\\nNormalization']\n",
    "scores = [sim_before, sim_after]\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "\n",
    "bars = ax.bar(categories, scores, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Lexical Similarity (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Impact of Normalization on Lexical Similarity\\n(Same Algorithm, Disguised Code)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 110)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "improvement = sim_after - sim_before\n",
    "print(f\"\\nðŸ“ˆ Similarity increase after normalization: +{improvement:.1f}%\")\n",
    "print(f\"   Normalization helps reveal hidden similarities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **Fast Computation**: TF-IDF is very efficient for quick screening\n",
    "\n",
    "âœ… **Supporting Evidence**: Good for confirming other signals\n",
    "\n",
    "âŒ **Weak Alone**: Cannot reliably detect algorithmic plagiarism\n",
    "\n",
    "âŒ **Fooled by Refactoring**: Low scores for identical algorithms with different style\n",
    "\n",
    "âš ï¸ **False Positives**: Similar syntax doesn't mean same logic\n",
    "\n",
    "**Key Insight**: Lexical similarity gets only **15% weight** in our system because it's easily fooled. We need structural and semantic analysis for robust detection!\n",
    "\n",
    "**Next Steps**: Proceed to notebook 03 to see the REAL power - AST vs RK-GST structural similarity!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
